{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import pyreadr\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read T1DGC data set\n",
    "t1dgc = pd.read_csv('T1DGCAllSNPs.csv')\n",
    "\n",
    "# remove the first unnecessary column\n",
    "t1dgc = t1dgc.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# drop unnecessary columns for Data pre-processing\n",
    "data = t1dgc[t1dgc.columns.drop(['sample.id', 'prediction', 'sex', 'HLA.DQB1_allele1', 'HLA.DQB1_allele2', 'DQB1', 'HLA.DRB1_allele1',\n",
    "       'HLA.DRB1_allele2', 'DRB1'])]\n",
    "\n",
    "# binarize labels column (affected: negative = 0, positive  = 1)\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "labels = lb.fit_transform(list(data.affected))\n",
    "\n",
    "# drop affected column, since we have already binarized labels\n",
    "data = data[data.columns.drop(['affected'])]\n",
    "\n",
    "\n",
    "# insert binarized labels as final column in our data set\n",
    "data.insert(len(data.columns), 'targets', labels, True) \n",
    "\n",
    "# separate features and target label in data set\n",
    "t1dgsData_AllSNPs_HIBAG_SNP_data = {'data' : data.loc[:, data.columns != 'targets'].values, 'target' : data['targets'].values}\n",
    "\n",
    "# save all SNP file\n",
    "with open('Data_AllSNPs_HIBAG_SNPS.pickle', 'wb') as f:\n",
    "    pickle.dump([t1dgsData_AllSNPs_HIBAG_SNP_data,data], f)\n",
    "    \n",
    "# with open('Data_AllSNPs_HIBAG_SNPS.pickle', 'rb') as f:\n",
    "#     t1dgsData_AllSNPs_HIBAG_SNP_data,data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with Nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation allows us to measure uncertainty and variance of model. It is more generalisable than traditional \n",
    "cross-validation. Nested CV composed of 10 outer and 10 inner folds. For every outer fold, fold is divided train / test set and training set is used with 10-fold traditional cross validation for hyperparameter tuning, and test set for performance assessment. Finally at the end of NestedCV there are 10 different models which we can measure variance of performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 10x10 Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "with open('Data_AllSNPs_HIBAG_SNPS.pickle', 'rb') as f:\n",
    "    t1dgsData_AllSNPs_HIBAG_SNP_data,data = pickle.load(f)\n",
    "\n",
    "# retrieve data and target labels\n",
    "X = t1dgsData_AllSNPs_HIBAG_SNP_data['data']\n",
    "y = t1dgsData_AllSNPs_HIBAG_SNP_data['target']\n",
    "\n",
    "\n",
    "\n",
    "# 10 Outer Loops\n",
    "split = 10\n",
    "cv_outer = StratifiedKFold(n_splits=split, shuffle = True, random_state = 42)\n",
    "cv_outer = cv_outer.split(X,y)\n",
    "train_list = []\n",
    "test_list = []\n",
    "for train,test in cv_outer:\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "    \n",
    "    \n",
    "# create a pipeline with feature scaling, then Logistic Regression\n",
    "pipe_sgd = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', LogisticRegression(solver = 'saga', penalty = 'l1', max_iter=10000,multi_class = 'auto', n_jobs = -1))])\n",
    "\n",
    "# Create parameters dictionary to look for optimal hyperparameters\n",
    "param_dist_sgd = {'clf__C': np.logspace(-5,2,10)}\n",
    "\n",
    "\n",
    "# Save train / test AUC scores and also selected feature coefficients for further analysis\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "tprs = list()\n",
    "Optims_list = list()\n",
    "test_AUCs = list()\n",
    "train_AUCS = list()\n",
    "test_roc = list()\n",
    "features_list = list()\n",
    "\n",
    "# Loop for every outer fold\n",
    "for i in range(split):\n",
    "    print(\"In progress: \", i+1,\"/\",split)\n",
    "    \n",
    "    # Inner loop: initialize 10-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=split, shuffle = True, random_state = 42)\n",
    "    \n",
    "    \n",
    "    # Construct GreedSearch with 10-folds cross-validation\n",
    "    sgd_randomized_pipe = GridSearchCV(estimator = pipe_sgd,param_grid=param_dist_sgd, \n",
    "                                       cv=cv, n_jobs=11, scoring = 'roc_auc', verbose = 3)\n",
    "    \n",
    "\n",
    "    # Train pipeline on GridSearch for Train data set\n",
    "    sgd_randomized_pipe.fit(X[train_list[i]], y[train_list[i]])\n",
    "    \n",
    "\n",
    "    # best optimal parameters\n",
    "    Optims = sgd_randomized_pipe.cv_results_['params'][sgd_randomized_pipe.best_index_]\n",
    "    Optims_list.append(Optims)\n",
    "    \n",
    "    \n",
    "    # Construct pipeline with optimal parameters\n",
    "    pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(C = Optims['clf__C'], solver = 'saga', \n",
    "                                                                              penalty = 'l1', max_iter=10000, multi_class = 'auto', n_jobs = 11))])\n",
    "\n",
    "    # Train pipeline on Train data\n",
    "    pipe_lr.fit(X[train_list[i]], y[train_list[i]])\n",
    "    \n",
    "    \n",
    "    # Train AUC \n",
    "    y_probs_train = pipe_lr.predict_proba(X[train_list[i]])[:,1]\n",
    "    fp_rate_train, tp_rate_train, thresholds = roc_curve(y[train_list[i]], y_probs_train)\n",
    "    train_auc = auc(fp_rate_train, tp_rate_train)\n",
    "    train_AUCS.append(train_auc)\n",
    "    \n",
    "    \n",
    "    # predict test score\n",
    "    y_probs_test = pipe_lr.predict_proba(X[test_list[i]])[:,1]\n",
    "    fp_rate_test, tp_rate_test, thresholds = roc_curve(y[test_list[i]], y_probs_test)\n",
    "    tprs.append(np.interp(mean_fpr, fp_rate_test, tp_rate_test))\n",
    "    \n",
    "    # store roc_aucs for plotting ROC curves later\n",
    "    roc_aucs = [fp_rate_test,tp_rate_test]\n",
    "    test_roc.append(roc_aucs)\n",
    "    \n",
    "    # test AUC \n",
    "    test_auc = auc(fp_rate_test, tp_rate_test)\n",
    "    test_AUCs.append(test_auc)\n",
    "    \n",
    "    \n",
    "    # Store selected features for further heatmap plotting\n",
    "    coef = pipe_lr.named_steps.clf.coef_\n",
    "    features = list(data.loc[:, data.columns != 'targets'].columns.values)\n",
    "    data_coeff = {'coefficient': list(abs(coef[0])), 'features': features}\n",
    "    data_coeff = pd.DataFrame(data=data_coeff)\n",
    "    data_coeff.sort_values(by=['coefficient'], inplace=True, ascending=False)\n",
    "    data_coeff = data_coeff[data_coeff['coefficient']!= 0]\n",
    "    features_list.append(data_coeff)\n",
    "    \n",
    "    \n",
    "# store variables for further usecase\n",
    "with open('AllSNPs_HIBAG_NestedCV.pickle', 'wb') as f:\n",
    "    pickle.dump([mean_fpr, tprs, Optims_list, train_AUCS, test_AUCs, test_roc, features_list], f)\n",
    "    \n",
    "# with open('AllSNPs_HIBAG_NestedCV.pickle', 'rb') as f:\n",
    "#     mean_fpr, tprs, Optims_list, train_AUCS, test_AUCs, test_roc, features_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curves for 10 different models, show variance / mean AUC and performance AUC for every model\n",
    "\n",
    "fig1 = plt.figure(figsize=[8,8])\n",
    "# Plot 45 degree line\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "\n",
    "for i in range(split):\n",
    "    plt.plot(test_roc[i][0], test_roc[i][1],label=r'Model: %d,(AUC = %0.2f )' % (i+1,test_AUCs[i]),lw=2)\n",
    "\n",
    "# compute m-str/m+std\n",
    "ci = np.std(tprs, axis = 0)\n",
    "\n",
    "# Compute mean of true positives and false positives\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "\n",
    "plt.fill_between(mean_fpr, (mean_tpr-3*ci), (mean_tpr+3*ci),color='blue', alpha=.1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves for Logistic Regression: AUCs std: %0.2f, AUCs mean: %0.2f' % (np.std(test_AUCs),np.mean(test_AUCs)))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient plots for selected features from Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine selected features for every model from Nested CV\n",
    "\n",
    "df = pd.merge(features_list[0],features_list[1],how = 'outer', on = 'features')\n",
    "\n",
    "for i in range(2,split):\n",
    "    df = pd.merge(df, features_list[i],how = 'outer', on = 'features')\n",
    "\n",
    "df.index = df['features'].values\n",
    "df = df.drop(['features'], axis=1)\n",
    "columns = list()\n",
    "for i in range(split):\n",
    "    model = 'model_{}'.format(str(i+1))\n",
    "    columns.append(model)\n",
    "df.columns = columns\n",
    "\n",
    "# since number of selected features might not be the same for every model\n",
    "# outer join will show NaNs for some features in specific models. \n",
    "# So, fill zero for NaN and round coefficients until 2 digits.\n",
    "\n",
    "df = df.fillna(0)\n",
    "df = df.round(2)\n",
    "df = df.T\n",
    "\n",
    "# plot heat map feature coefficients from Nested CV\n",
    "fig = plt.figure(figsize=[30,8])\n",
    "chart = sns.heatmap(df, annot=False)\n",
    "chart.set_title('Heatmap for the feature coefficients', fontsize=25)\n",
    "chart.set_ylabel('Best models from NestedCV', fontsize=20)\n",
    "chart.set_xlabel('Features', fontsize=20)\n",
    "y = chart.set_yticklabels(chart.get_yticklabels(), rotation=0)\n",
    "y = chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC vs Top features for every model in Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC vs Top ranked features allows us to observe how every model behaves in different parts of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('Data_AllSNPs_HIBAG_SNPS.pickle', 'rb') as f:\n",
    "    t1dgsData_AllSNPs_HIBAG_SNP_data,data_old = pickle.load(f)\n",
    "\n",
    "# initialize lists to store models performances\n",
    "models = list()\n",
    "intervals = list()\n",
    "\n",
    "# for every test set of outer folds, run respective best model of that fols on \n",
    "# by adding the top features one by one\n",
    "for i in range(split):\n",
    "    print(\"In progress: {} / {}\".format(i+1,split))\n",
    "    \n",
    "    # retrieve Optims\n",
    "    Optims = Optims_list[i]\n",
    "\n",
    "    # top Features from Lasso\n",
    "    data_coeff = features_list[i]\n",
    "    top_ranked_features = list(data_coeff[data_coeff['coefficient'] != 0].features.values)\n",
    "\n",
    "    labels = data_old['targets'].values\n",
    "\n",
    "    # Select data frame with top ranked features\n",
    "    data = pd.DataFrame(data_old, columns = top_ranked_features)\n",
    "\n",
    "    # insert binarized labels as final column in our data set\n",
    "    data.insert(len(data.columns), 'targets', labels, True) \n",
    "\n",
    "    # separate features and target label in data set\n",
    "    t1dgsData = {'data' : data.loc[:, data.columns != 'targets'].values, 'target' : data['targets'].values}\n",
    "\n",
    "    X = t1dgsData['data']\n",
    "    y = t1dgsData['target']\n",
    "    X_train = X[train_list[i]]\n",
    "    y_train = y[train_list[i]]\n",
    "    X_test = X[test_list[i]]\n",
    "    y_test = y[test_list[i]]\n",
    "    \n",
    "    \n",
    "    # loop for every top-ranked feature and train it on train set of respective fold and \n",
    "    # measure performance on test set\n",
    "    n_features = X_train.shape[1]\n",
    "    testAuc = list()\n",
    "    for feature in range(n_features):\n",
    "        \n",
    "        # create a pipeline with optimal parameters\n",
    "        pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(C = Optims['clf__C'], solver = 'saga', \n",
    "                                                                              penalty = 'l1', max_iter=10000, multi_class = 'auto', n_jobs = 11))])\n",
    "        # Train pipeline on Train data\n",
    "        pipe_lr.fit(X_train[:,:feature+1], y_train)\n",
    "\n",
    "        # Test AUC on trained model\n",
    "        y_probs_test = pipe_lr.predict_proba(X_test[:,:feature+1])[:,1]\n",
    "        fp_rate_test, tp_rate_test, thresholds = roc_curve(y_test, y_probs_test)\n",
    "        test_auc = auc(fp_rate_test, tp_rate_test)\n",
    "        testAuc.append(test_auc)\n",
    "    \n",
    "    models.append(testAuc)\n",
    "    x_axis = np.linspace(1, X_train.shape[1]+1, X_train.shape[1])\n",
    "    intervals.append(x_axis)\n",
    "    \n",
    "# store variables for further usecase\n",
    "with open('AllSNPs_HIBAG_AUCvsTopFeatures_NestedCV.pickle', 'wb') as f:\n",
    "    pickle.dump([intervals, models], f)\n",
    "    \n",
    "# with open('AllSNPs_HIBAG_AUCvsTopFeatures_NestedCV.pickle', 'rb') as f:\n",
    "#     intervals, models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUCs vs Top-ranked features for every model from Nested CV\n",
    "fig3 = plt.figure(figsize=[8,8])\n",
    "for i in range(split):\n",
    "    plt.plot(intervals[i],models[i],label=\"Model: %d\" % (i+1),lw=2)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(\"NestedCV: Top ranked features vs AUC\" )\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"Top Ranked Features\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
